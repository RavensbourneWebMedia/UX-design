# Session 7

### Today, Tuesday 24th February 2015

1. User-testing	
* Guest lecturer: [Amy Jackson-Bruce](https://www.linkedin.com/in/amyjacksonbruce)	
	
# A reminder
	
This is a [checklist](https://github.com/RavensbourneWebMedia/WEB14204/blob/master/sessions/session-09.md#checklist-for-presentations) of what we want you to present during your formative assessment (in 2 weeks' time).


# User-testing

### Why?

* Because we can't base a design on assumptions only, we need some evidence
* Because when we work on something (anything) we gradually lose a fresh, unbiased view of it
* Because people may have different mental models from ours
* Because we need to spot and correct problems as early as possible.

### When?

* End of design?
* Once you're live?
* **Now**?

### How many?

* Do you think itâ€™s more effective to test 100 people or 10?

* Is it better to test 100 people once or 10 people twice? 

<!-- Test many times, few people at a time -->

User-testing is **iterative**, you should do small tests as often as possible, not just once.

### Who?

Ideally you'd pick someone who's **representative** of your target audience, but following the principle that *some testing is way better than no testing* you can pick anyone really: friends, relatives and colleagues.


## Types of user-testing

### Quantitative vs qualitative

* **Quantitative** testing is used to observe and measure behaviour, generally resulting in statistical data. 

	**Surveys**, **tree testing** and **analytics** are quantitative methods. 

* **Qualitative** testing is used to gather insights on what users are experiencing when they behave or why.

	**Interviews** and **lab testing** are the most common qualitative methods. 	

### Remote vs face2face

* Face2face testing is held with the user in person. 

	Ideally you would have a *facilitator* conduct the test while you observe and take notes. 

	This is because it's tricky not to *feel personally attached* to something you've made, also you may be tempted to *give directions* to users and they may feel *less inclided to express their honest feelings* if they know you've made the thing they're testing. 

	However, affording a facilitator is not always possible so you may have to be the facilitator of your own face2face tests.

	It's vital to **take notes** and highly recommended to **record** user-tests. 

	You can use [Silverback](http://silverbackapp.com) to video-record your sessions on any device with a front-facing camera. The result is a video that shows you both the screen people are interacting with, and they're faces while they're testing (and their voices too).

	[Download Silverback](http://silverback.s3.amazonaws.com/silverback2.zip)

* In remote testing respondents are always contacted remotely, usually via the Web.


Most types of testing can be done both remotely and face2face.


# Warm-up exercise

Let's start with some quick and dirty remote testing.

1. Sign up for a free account on [UsabilityHub](https://usabilityhub.com/?r=30141), take a few tests to see what it's like and get inspired about what you can test. 

2. Create one test (or more if you feel like). You can choose between 3 types of test:

	* **Five Second** to capture your users' first impressions
	* **Click Test** to understand what your users click
	* **Nav Flow** to see if your users can successfully get from A to B

3. Make the test public (private tests are not free, anyway) and then share the URL with your friends on Facebook, with your followers on Twitter etc. 

4. Your mission is to get at least 20 tests by the end of today. Can you do that?

	
# Assignments

1. Blog about the results of your remote testing

2. Conduct a face2face user-test of your wireframes using [Silverback](http://silverbackapp.com/)



## TODO

